env:
  mode: coupled
  dt: 1.0
  frame_stack: 1
  action_type: angles_thrust       # Use Shuttlecock adaptor with eta1, eta2, thrust
  # Control cadence: actions per orbit (increase for better learning signal)
  per_orbit_steps: 60
  # Controls (match C-side limits)
  eta_limit_rad: 1.91986217719     # 110 deg in radians
  thrust_max_N: 0.05               # typical max thrust
  gate_thrust_in_eclipse: true
  # Target orbital parameters for rewards
  target_ecc: 0.0                  # target eccentricity

  # Observation scaling (kept simple/default-like)
  obs_scales: {dr: 5000.0, dv: 0.1, da: 100.0, e: 0.01, theta_deg: 20.0, omega: 0.1}

# Reward weights (only the following are used by the current reward function)
reward_weights:
  # Altitude in meters can be large; scale down to keep per-step magnitudes ~O(1â€“10)
  w_a: 5.0e-6
  # Eccentricity is dimensionless in [0,1]
  w_e: 1.0
  # Quaternion-based nadir pointing term
  w_theta: 1.0

# Training hyperparameters and rollout cadence
training:
  num_envs: 16
  # Per-episode timing (10 orbits, 60 steps/orbit)
  episode_orbits: 10
  steps_per_orbit: 60
  # Update after N episodes per env (rollout length = episode_steps * N)
  update_every_episodes: 1
  n_epochs: 10
  learning_rate: 0.0003
  total_steps: 1000000            # plan for longer training with denser control
  tb_logdir: runs/tb
